{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcbf0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from -r requirements (line 1)) (0.3.13)\n",
      "Requirement already satisfied: matplotlib in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from -r requirements (line 2)) (3.10.6)\n",
      "Requirement already satisfied: scikit-learn in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from -r requirements (line 3)) (1.7.2)\n",
      "Requirement already satisfied: numpy in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from -r requirements (line 4)) (2.2.6)\n",
      "Requirement already satisfied: packaging in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from kagglehub->-r requirements (line 1)) (25.0)\n",
      "Requirement already satisfied: requests in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from kagglehub->-r requirements (line 1)) (2.32.5)\n",
      "Requirement already satisfied: pyyaml in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from kagglehub->-r requirements (line 1)) (6.0.2)\n",
      "Requirement already satisfied: tqdm in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from kagglehub->-r requirements (line 1)) (4.67.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from matplotlib->-r requirements (line 2)) (4.59.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from matplotlib->-r requirements (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from matplotlib->-r requirements (line 2)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from matplotlib->-r requirements (line 2)) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from matplotlib->-r requirements (line 2)) (3.2.3)\n",
      "Requirement already satisfied: pillow>=8 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from matplotlib->-r requirements (line 2)) (11.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from matplotlib->-r requirements (line 2)) (1.4.9)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from scikit-learn->-r requirements (line 3)) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from scikit-learn->-r requirements (line 3)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from scikit-learn->-r requirements (line 3)) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements (line 2)) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from requests->kagglehub->-r requirements (line 1)) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from requests->kagglehub->-r requirements (line 1)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from requests->kagglehub->-r requirements (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/nov/Documents/NSU2025/Dev_based_NN/.venv/lib/python3.10/site-packages (from requests->kagglehub->-r requirements (line 1)) (3.4.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688bbcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Execution\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 138\u001b[0m\n\u001b[1;32m    134\u001b[0m     print_vector_example(X, vectorizer, feature_names, text_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 126\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExecution\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m() \u001b[38;5;66;03m# This is the list of all documents in string format\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m documents_to_list(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mabs_rel_path[\u001b[38;5;241m0\u001b[39m], args\u001b[38;5;241m.\u001b[39mdata_dir_name[\u001b[38;5;241m0\u001b[39m], texts)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Vectorize texts\u001b[39;00m\n\u001b[1;32m    129\u001b[0m X, vectorizer, feature_names \u001b[38;5;241m=\u001b[39m text_vectorizing(texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import argparse\n",
    "\n",
    "\n",
    "def download_data(type_of_path, dir_name):\n",
    "    \"\"\"\n",
    "    Downloading of dataset from kaggle for solution(jensenbaxter/10dataset-text-document-classification)\n",
    "    \"\"\"\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"jensenbaxter/10dataset-text-document-classification\")\n",
    "    if type_of_path == 'r':\n",
    "        shutil.copytree(path, \"./\" + dir_name, dirs_exist_ok=True)\n",
    "    else:\n",
    "        shutil.copytree(path, dir_name, dirs_exist_ok=True)\n",
    "   \n",
    "\n",
    "def documents_to_list(abs_rel_path : str, dir_name : str, texts : list):\n",
    "    \"\"\"\n",
    "    Make the list of all documents from dataset\n",
    "    \"\"\"\n",
    "    if abs_rel_path == 'a':\n",
    "        dir_names = os.listdir(dir_name)\n",
    "    else:\n",
    "        dir_names = os.listdir(dir_name)\n",
    "    for dir in dir_names:\n",
    "        filenames = os.listdir(os.path.abspath(dir_name) + \"/\" + dir)\n",
    "        for filename in filenames:\n",
    "            with open((os.path.abspath(dir_name) + \"/\" + dir + \"/\" + filename), \"r\") as file:\n",
    "                texts.append(file.read())\n",
    "\n",
    "\n",
    "def text_vectorizing(texts):\n",
    "    \"\"\"\n",
    "    Creation of token matrix for all documents\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Get feature names (vocabulary)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"Vocabulary size: {len(feature_names)} words\")\n",
    "    \n",
    "    return X, vectorizer, feature_names\n",
    "\n",
    "\n",
    "def Statistics(X):\n",
    "    \"\"\"\n",
    "    Show result of vectorizing. Amount of elements in matrix, non-zero elements, zero elements, Sparse and Density of matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    total_elements = X.shape[0] * X.shape[1]  # Всего элементов в матрице\n",
    "    non_zero_elements = X.nnz                 # Ненулевых элементов\n",
    "    zero_elements = total_elements - non_zero_elements # Нулевых элементов\n",
    "\n",
    "    sparsity_ratio = zero_elements / total_elements\n",
    "    density_ratio = non_zero_elements / total_elements\n",
    "\n",
    "\n",
    "    print(\"Statistics: \\n\")\n",
    "    print(f\"Всего элементов в матрице: {total_elements}\")\n",
    "    print(f\"Ненулевых элементов: {non_zero_elements}\")\n",
    "    print(f\"Нулевых элементов: {zero_elements}\")\n",
    "    print(f\"Разреженность (доля нулей): {sparsity_ratio:.2%}\")\n",
    "    print(f\"Плотность (доля не нулей): {density_ratio:.2%}\")\n",
    "\n",
    "\n",
    "def print_vector_example(X, vectorizer, feature_names, text_index=1):\n",
    "    \"\"\"\n",
    "    Print human-readable representation of one vector\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Analysis of vector for document {text_index} ===\")\n",
    "    \n",
    "    # Get the specific vector as a dense array\n",
    "    vector_dense = X[text_index].toarray()[0]\n",
    "    \n",
    "    # Get non-zero elements and their values\n",
    "    non_zero_indices = X[text_index].indices\n",
    "    non_zero_values = X[text_index].data\n",
    "    \n",
    "    print(f\"Vector shape: {X[text_index].shape}\")\n",
    "    print(f\"Non-zero elements: {len(non_zero_values)}\")\n",
    "    print(f\"Total elements: {X.shape[1]}\")\n",
    "    \n",
    "    print(\"\\nFirst 20 non-zero elements with word names:\")\n",
    "    for i, (idx, value) in enumerate(zip(non_zero_indices[:20], non_zero_values[:20])):\n",
    "        word = feature_names[idx]\n",
    "        print(f\"  '{word}': {value}\")\n",
    "    \n",
    "    if len(non_zero_values) > 20:\n",
    "        print(f\"  ... and {len(non_zero_values) - 20} more words\")\n",
    "    \n",
    "    # Show some statistics about this vector\n",
    "    print(f\"\\nVector statistics:\")\n",
    "    print(f\"  Total words in document: {sum(non_zero_values)}\")\n",
    "    print(f\"  Unique words in document: {len(non_zero_values)}\")\n",
    "    print(f\"  Most frequent word: '{feature_names[non_zero_indices[np.argmax(non_zero_values)]]}' \" \n",
    "          f\"({max(non_zero_values)} occurrences)\")\n",
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description='Process directory to save data')\n",
    "    # parser.add_argument('--abs_rel_path', type=str, nargs=1, default='r',\n",
    "    #                     help='using of absolute or relative path type \\'a\\' to use absolute and \\'r\\' to use relative,\\n \\'r\\' is default')\n",
    "    # parser.add_argument('data_dir_name', type=str, nargs=1,\n",
    "    #                     help='name of directory which will be using to save data')\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # if args.abs_rel_path[0] != 'a' and args.abs_rel_path[0] != 'r':\n",
    "    #     print(\"Please enter correct type of path. Use --help to see options.\")\n",
    "    #     return\n",
    "\n",
    "    \n",
    "\n",
    "    download_data('r', 'data')\n",
    "    \n",
    "    print(\"\\n\\nExecution\\n\")\n",
    "\n",
    "    texts = list() # This is the list of all documents in string format\n",
    "\n",
    "    documents_to_list('r', 'data', texts)\n",
    "\n",
    "    # Vectorize texts\n",
    "    X, vectorizer, feature_names = text_vectorizing(texts)\n",
    "\n",
    "    Statistics(X)\n",
    "\n",
    "    # Print detailed analysis of one vector\n",
    "    print_vector_example(X, vectorizer, feature_names, text_index=1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504f773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
